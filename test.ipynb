{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96492771-2f40-4016-a9bb-759ef7a7633e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in c:\\users\\rismosch\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\rismosch\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\rismosch\\appdata\\roaming\\python\\python310\\site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rismosch\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rismosch\\appdata\\roaming\\python\\python310\\site-packages (from tqdm) (0.4.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\rismosch\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rismosch\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\rismosch\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rismosch\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rismosch\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce27439a-de90-41a8-97c9-59e746bee240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "url = \"https://books.toscrape.com\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text)\n",
    "\n",
    "all_current = soup.find_all(\"li\", {\"class\": \"current\"})\n",
    "current = all_current[0]\n",
    "encoded = current.encode_contents().decode(\"utf-8\")\n",
    "splits = encoded.split(\"\\n\")\n",
    "page_string = splits[2].strip()\n",
    "splits = page_string.split(\"of\")\n",
    "page_count_string = splits[1].strip()\n",
    "page_count = int(page_count_string)\n",
    "\n",
    "page_min = page_count - 9\n",
    "page_max = page_count + 1\n",
    "\n",
    "link_list = []\n",
    "for i in range(page_min, page_max):\n",
    "    page = f'https://books.toscrape.com/catalogue/page-{i}.html'\n",
    "    response = requests.get(page)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "    \n",
    "    for i in soup.findAll(\"h3\"):\n",
    "        for link in i.find_all('a',href = True):\n",
    "            part_link = link[\"href\"]                                  \n",
    "            full_link = f'https://books.toscrape.com/catalogue/{part_link}'\n",
    "            link_list.append(full_link)\n",
    "\n",
    "print(len(link_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ddfba652-e98b-467a-a702-a8e53511421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/200\n",
      "2/200\n",
      "3/200\n",
      "4/200\n",
      "5/200\n",
      "6/200\n",
      "7/200\n",
      "8/200\n",
      "9/200\n",
      "10/200\n",
      "11/200\n",
      "12/200\n",
      "13/200\n",
      "14/200\n",
      "15/200\n",
      "16/200\n",
      "17/200\n",
      "18/200\n",
      "19/200\n",
      "20/200\n",
      "21/200\n",
      "22/200\n",
      "23/200\n",
      "24/200\n",
      "25/200\n",
      "26/200\n",
      "27/200\n",
      "28/200\n",
      "29/200\n",
      "30/200\n",
      "31/200\n",
      "32/200\n",
      "33/200\n",
      "34/200\n",
      "35/200\n",
      "36/200\n",
      "37/200\n",
      "38/200\n",
      "39/200\n",
      "40/200\n",
      "41/200\n",
      "42/200\n",
      "43/200\n",
      "44/200\n",
      "45/200\n",
      "46/200\n",
      "47/200\n",
      "48/200\n",
      "49/200\n",
      "50/200\n",
      "51/200\n",
      "52/200\n",
      "53/200\n",
      "54/200\n",
      "55/200\n",
      "56/200\n",
      "57/200\n",
      "58/200\n",
      "59/200\n",
      "60/200\n",
      "61/200\n",
      "62/200\n",
      "63/200\n",
      "64/200\n",
      "65/200\n",
      "66/200\n",
      "67/200\n",
      "68/200\n",
      "69/200\n",
      "70/200\n",
      "71/200\n",
      "72/200\n",
      "73/200\n",
      "74/200\n",
      "75/200\n",
      "76/200\n",
      "77/200\n",
      "78/200\n",
      "79/200\n",
      "80/200\n",
      "81/200\n",
      "82/200\n",
      "83/200\n",
      "84/200\n",
      "85/200\n",
      "86/200\n",
      "87/200\n",
      "88/200\n",
      "89/200\n",
      "90/200\n",
      "91/200\n",
      "92/200\n",
      "93/200\n",
      "94/200\n",
      "95/200\n",
      "96/200\n",
      "97/200\n",
      "98/200\n",
      "99/200\n",
      "100/200\n",
      "101/200\n",
      "102/200\n",
      "103/200\n",
      "104/200\n",
      "105/200\n",
      "106/200\n",
      "107/200\n",
      "108/200\n",
      "109/200\n",
      "110/200\n",
      "111/200\n",
      "112/200\n",
      "113/200\n",
      "114/200\n",
      "115/200\n",
      "116/200\n",
      "117/200\n",
      "118/200\n",
      "119/200\n",
      "120/200\n",
      "121/200\n",
      "122/200\n",
      "123/200\n",
      "124/200\n",
      "125/200\n",
      "126/200\n",
      "127/200\n",
      "128/200\n",
      "129/200\n",
      "130/200\n",
      "131/200\n",
      "132/200\n",
      "133/200\n",
      "134/200\n",
      "135/200\n",
      "136/200\n",
      "137/200\n",
      "138/200\n",
      "139/200\n",
      "140/200\n",
      "141/200\n",
      "142/200\n",
      "143/200\n",
      "144/200\n",
      "145/200\n",
      "146/200\n",
      "147/200\n",
      "148/200\n",
      "149/200\n",
      "150/200\n",
      "151/200\n",
      "152/200\n",
      "153/200\n",
      "154/200\n",
      "155/200\n",
      "156/200\n",
      "157/200\n",
      "158/200\n",
      "159/200\n",
      "160/200\n",
      "161/200\n",
      "162/200\n",
      "163/200\n",
      "164/200\n",
      "165/200\n",
      "166/200\n",
      "167/200\n",
      "168/200\n",
      "169/200\n",
      "170/200\n",
      "171/200\n",
      "172/200\n",
      "173/200\n",
      "174/200\n",
      "175/200\n",
      "176/200\n",
      "177/200\n",
      "178/200\n",
      "179/200\n",
      "180/200\n",
      "181/200\n",
      "182/200\n",
      "183/200\n",
      "184/200\n",
      "185/200\n",
      "186/200\n",
      "187/200\n",
      "188/200\n",
      "189/200\n",
      "190/200\n",
      "191/200\n",
      "192/200\n",
      "193/200\n",
      "194/200\n",
      "195/200\n",
      "196/200\n",
      "197/200\n",
      "198/200\n",
      "199/200\n",
      "200/200\n",
      "                                                  name            category  \\\n",
      "0    Notes from a Small Island (Notes From a Small ...          nonfiction   \n",
      "1                         Night (The Night Trilogy #1)          nonfiction   \n",
      "2            Neither Here nor There: Travels in Europe              travel   \n",
      "3                                                Naked               humor   \n",
      "4                         Morning Star (Red Rising #3)             default   \n",
      "..                                                 ...                 ...   \n",
      "195  Alice in Wonderland (Alice's Adventures in Won...            classics   \n",
      "196   Ajin: Demi-Human, Volume 1 (Ajin: Demi-Human #1)      sequential-art   \n",
      "197  A Spy's Devotion (The Regency Spies of London #1)  historical-fiction   \n",
      "198                1st to Die (Women's Murder Club #1)             mystery   \n",
      "199                 1,000 Places to See Before You Die              travel   \n",
      "\n",
      "     price  availability  \n",
      "0    40.17             3  \n",
      "1    13.51             3  \n",
      "2    38.95             3  \n",
      "3    31.69             3  \n",
      "4    29.40             3  \n",
      "..     ...           ...  \n",
      "195  55.53             1  \n",
      "196  57.06             1  \n",
      "197  16.97             1  \n",
      "198  53.98             1  \n",
      "199  26.08             1  \n",
      "\n",
      "[200 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "dictionary_list = []\n",
    "counter = 0\n",
    "length = len(link_list)\n",
    "for page in link_list:\n",
    "    counter += 1;\n",
    "    print(f'{counter}/{length}') # drucke den fortschritt aus, damit man sieht das was passiert\n",
    "    response = requests.get(page)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    header = soup.find_all(\"h1\")[0] # hole den ersten (einzigsten) header\n",
    "    name = header.encode_contents().decode(\"utf-8\") # erstelle einen string aus dem header\n",
    "\n",
    "    is_first = True\n",
    "    category_link = \"\"\n",
    "    for a in soup.find_all('a', href=True): # finde alle a\n",
    "        href = a['href']\n",
    "        if \"category\" in href: # finde alle category links (2)\n",
    "            if is_first:\n",
    "                is_first = False # wenn der erste link gefunden wurde, setze is_second auf False\n",
    "            else:\n",
    "                category_link = href # zweiter category link gefunden!\n",
    "                break # wir können die loop beenden, weil wir den link gefunden haben\n",
    "\n",
    "    splits = category_link.split(\"/\")\n",
    "    category = splits[3].split(\"_\")[0] # hole die category aus dem link\n",
    "\n",
    "    price = 0;\n",
    "    availability = 0;\n",
    "\n",
    "    keys = soup.find_all(\"th\"); # hole alle produkt informations keys\n",
    "    values = soup.find_all(\"td\"); # hole alle produkt informations values\n",
    "    zipped = zip(keys, values); # füge keys und values zusammen\n",
    "\n",
    "    for key, value in zipped: # iteriere zipped\n",
    "        key_string = key.encode_contents().decode(\"utf-8\") # erstelle einen string aus dem html element\n",
    "        value_string = value.encode_contents().decode(\"utf-8\") # erstelle einen string aus dem html element\n",
    "\n",
    "        # print(f'{key_string} -> {value_string}')\n",
    "\n",
    "        if \"Price (incl. tax)\" in key_string: # wenn price, dann hole die float aus dem string\n",
    "            price_string = value_string[2:]\n",
    "            price = float(price_string)\n",
    "        elif \"Availability\" in key_string: # wenn availability, hole den int aus dem string\n",
    "            splits = value_string.split(\"(\")\n",
    "            splits = splits[1].split(\" \")\n",
    "            availabilty_string = splits[0]\n",
    "            availability = int(availabilty_string)\n",
    "\n",
    "    # generiere dictionary\n",
    "    dictionary = {\n",
    "        \"name\": name,\n",
    "        \"category\": category,\n",
    "        \"price\": price,\n",
    "        \"availability\": availability,\n",
    "    }\n",
    "    \n",
    "    dictionary_list.append(dictionary)\n",
    "\n",
    "# erstelle dataframe\n",
    "data_frame = pd.DataFrame(data=dictionary_list)\n",
    "print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b8f0f636-219b-49f3-b23e-b0148bdafec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excercise 2\n"
     ]
    }
   ],
   "source": [
    "print(\"excercise 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "246c625a-77df-4e7a-9a2a-3eb08fed8e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 981 laureates\n",
      "fetching... https://masterdataapi.nobelprize.org/2.1/laureates?sort=asc&offset=0&limit=981\n",
      "parsing to json...\n",
      "981\n"
     ]
    }
   ],
   "source": [
    "# get only 1 laureate, so that we can access meta.count, to know the total amount of laureates\n",
    "initial_url = \"https://masterdataapi.nobelprize.org/2.1/laureates?offset=0&limit=1\"\n",
    "response = requests.get(initial_url)\n",
    "data = response.json()\n",
    "meta = data['meta']\n",
    "total_laureates_count = meta['count']\n",
    "\n",
    "print(f'found {total_laureates_count} laureates')\n",
    "\n",
    "# get ALL laureates\n",
    "url = f'https://masterdataapi.nobelprize.org/2.1/laureates?sort=asc&offset=0&limit={total_laureates_count}'\n",
    "response = requests.get(url);\n",
    "data = response.json()\n",
    "all_laureates = data['laureates']\n",
    "\n",
    "print(len(all_laureates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8de4f794-b97d-405a-b138-6177642bde0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n"
     ]
    }
   ],
   "source": [
    "laureates = []\n",
    "\n",
    "for laureate in all_laureates:\n",
    "    if \"fullName\" in laureate: # if laureate doesn't contain \"fullName\", it is not a person\n",
    "        name = laureate['fullName']['en']\n",
    "        birthday = laureate['birth']['date']\n",
    "        nobel_prizes = []\n",
    "        \n",
    "        for nobel_prize in laureate['nobelPrizes']:\n",
    "            dateAwarded = \"\"\n",
    "            if 'dateAwarded' in nobel_prize: # if 'dateAwarded' doesn't exist, use 'awardYear'\n",
    "                dateAwarded = nobel_prize['dateAwarded']\n",
    "            else:\n",
    "                dateAwarded = nobel_prize['awardYear']\n",
    "            \n",
    "            dictionary = {\n",
    "                'dateAwarded': dateAwarded,\n",
    "                'category': nobel_prize['category']['en'],\n",
    "            }\n",
    "            \n",
    "            nobel_prizes.append(dictionary)\n",
    "        \n",
    "        birth_country = \"unknown\"\n",
    "        if 'place' in laureate['birth']: # if birthplace exists, take it, otherwise it will be \"unkown\"\n",
    "            birth_country = laureate['birth']['place']['country']['en']\n",
    "        \n",
    "        wikipedia = laureate['wikipedia']['english']\n",
    "        \n",
    "        # create dictionary and add to list\n",
    "        dictionary = {\n",
    "            'name': name,\n",
    "            'birthday': birthday,\n",
    "            'nobel_prizes': nobel_prizes,\n",
    "            'birth_country': birth_country,\n",
    "            'wikipedia': wikipedia\n",
    "        }\n",
    "        \n",
    "        laureates.append(dictionary)\n",
    "\n",
    "print(len(laureates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f289fa0-d3f5-4711-955b-4311a54fce15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
